services:

  # llm:
  #   image: sinanozel/ollama.0.12.2:llava-7b
  #   ports:
  #     - "11434:11434"
  #   networks:
  #     - nutrition-information-extraction-evaluation
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           capabilities: ["gpu"]
  #           count: all

  embedding:
    image: sinanozel/ollama.0.12.2:all-minilm-33m
    networks:
      - model-evaluation

  evaluator:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - HF_API_KEY=${HF_API_KEY}
      - PROJECT_PATH=/${PROJECT_PATH}
      - EVALUATION_PATH=${EVALUATION_PATH:-evaluation}
    depends_on:
      # - llm
      - embedding
    networks:
      - model-evaluation
    tty: true
    volumes:
      - ./${PROJECT_PATH}:/${PROJECT_PATH}
      - ./test_model_evaluation.py:/test_model_evaluation.py
      - ./conftest.py:/conftest.py
      - ./helpers:/helpers
    command: ["pytest", "/test_model_evaluation.py", "--disable-warnings", "-vvv", "--full-trace", "-rs", "-s"]

networks:
  model-evaluation:
    driver: bridge