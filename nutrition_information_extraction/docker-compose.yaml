services:

  llm:
    image: sinanozel/ollama.0.12.2:llava-7b
    ports:
      - "11434:11434"
    networks:
      - nutrition-information-extraction-evaluation
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all

  embedding:
    image: sinanozel/ollama.0.12.2:all-minilm-33m
    networks:
      - nutrition-information-extraction-evaluation

  evaluator:
    build:
      context: ..
      dockerfile: Dockerfile
    environment:
      - OLLAMA_URL=http://llm:11434
      - OLLAMA_MODELS=ollama/llava:7b
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_MODELS=mistral/pixtral-12b-2409
      # - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      # - ANTHROPIC_MODELS=anthropic/claude-sonnet-4-5-20250929,anthropic/claude-haiku-4-5-20251001
      # - HF_API_KEY=${HF_API_KEY}
      # - HF_MODELS=huggingface/sambanova/meta-llama/Llama-3.2-11B-Vision-Instruct
    depends_on:
      - llm
      - embedding
    networks:
      - nutrition-information-extraction-evaluation
    tty: true
    volumes:
      - ./tests:/tests

networks:
  nutrition-information-extraction-evaluation:
    driver: bridge